# kubectl 命令

```bash
# 基础命令，从文件，控制台创建资源，暴露service, controller, deployment,pod服务，run一个镜像，修改指定对象的特征属性
Basic Commands (Beginner):
  create         Create a resource from a file or from stdin.
  expose         Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service
  run            Run a particular image on the cluster
  set            Set specific features on objects
# 中级基础命令 
Basic Commands (Intermediate):
  explain        Documentation of resources
  get            Display one or many resources
  edit           Edit a resource on the server
  delete         Delete resources by filenames, stdin, resources and names, or by resources and label selector
# 发布相关的命令，回滚，扩容，自动扩容
Deploy Commands:
  rollout        Manage the rollout of a resource
  scale          Set a new size for a Deployment, ReplicaSet or Replication Controller
  autoscale      Auto-scale a Deployment, ReplicaSet, or ReplicationController
# 集群管理相关的 证书，集群信息，top资源使用情况，标记资源是否可调度，污点
Cluster Management Commands:
  certificate    Modify certificate resources.
  cluster-info   Display cluster info
  top            Display Resource (CPU/Memory/Storage) usage.
  cordon         Mark node as unschedulable
  uncordon       Mark node as schedulable
  drain          Drain node in preparation for maintenance
  taint          Update the taints on one or more nodes
# 问题定位命令 
Troubleshooting and Debugging Commands:
  describe       Show details of a specific resource or group of resources
  logs           Print the logs for a container in a pod
  attach         Attach to a running container
  exec           Execute a command in a container
  port-forward   Forward one or more local ports to a pod
  proxy          Run a proxy to the Kubernetes API server
  cp             Copy files and directories to and from containers.
  auth           Inspect authorization
# 高级命令
Advanced Commands:
  diff           Diff live version against would-be applied version
  apply          Apply a configuration to a resource by filename or stdin
  patch          Update field(s) of a resource using strategic merge patch
  replace        Replace a resource by filename or stdin
  wait           Experimental: Wait for a specific condition on one or many resources.
  convert        Convert config files between different API versions
  kustomize      Build a kustomization target from a directory or a remote url.
# 设置命令 打标签，注解，shell自动补全
Settings Commands:
  label          Update the labels on a resource
  annotate       Update the annotations on a resource
  completion     Output shell completion code for the specified shell (bash or zsh)
# 其他命令，api所支持的资源，修改kubeconfig，插件等
Other Commands:
  api-resources  Print the supported API resources on the server
  api-versions   Print the supported API versions on the server, in the form of "group/version"
  config         Modify kubeconfig files
  plugin         Provides utilities for interacting with plugins.
  version        Print the client and server version information

```

# 试验1

* run 一个 `docker run nginx --image=nginx`

## kubectl run的参数

```
# Start a single instance of nginx.
  kubectl run nginx --image=nginx

  # Start a single instance of hazelcast and let the container expose port 5701 .
  kubectl run hazelcast --image=hazelcast --port=5701

  # Start a single instance of hazelcast and set environment variables "DNS_DOMAIN=cluster" and "POD_NAMESPACE=default"
in the container.
  kubectl run hazelcast --image=hazelcast --env="DNS_DOMAIN=cluster" --env="POD_NAMESPACE=default"

  # Start a single instance of hazelcast and set labels "app=hazelcast" and "env=prod" in the container.
  kubectl run hazelcast --image=hazelcast --labels="app=hazelcast,env=prod"

  # Start a replicated instance of nginx.
  kubectl run nginx --image=nginx --replicas=5

  # Dry run. Print the corresponding API objects without creating them.
  kubectl run nginx --image=nginx --dry-run

  # Start a single instance of nginx, but overload the spec of the deployment with a partial set of values parsed from
JSON.
  kubectl run nginx --image=nginx --overrides='{ "apiVersion": "v1", "spec": { ... } }'

  # Start a pod of busybox and keep it in the foreground, don't restart it if it exits.
  kubectl run -i -t busybox --image=busybox --restart=Never

  # Start the nginx container using the default command, but use custom arguments (arg1 .. argN) for that command.
  kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN>

  # Start the nginx container using a different command and custom arguments.
  kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>

  # Start the perl container to compute π to 2000 places and print it out.
  kubectl run pi --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(2000)'

  # Start the cron job to compute π to 2000 places and print it out every 5 minutes.
  kubectl run pi --schedule="0/5 * * * ?" --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(2000)'
```

kubectl run 是自动帮创建一个deployment

## kubectl expose

> 自动创建 service

```bash
Expose a resource as a new Kubernetes service.

 Looks up a deployment, service, replica set, replication controller or pod by name and uses the selector for that
resource as the selector for a new service on the specified port. A deployment or replica set will be exposed as a
service only if its selector is convertible to a selector that service supports, i.e. when the selector contains only
the matchLabels component. Note that if no port is specified via --port and the exposed resource has multiple ports, all
will be re-used by the new service. Also if no labels are specified, the new service will re-use the labels from the
resource it exposes.

 Possible resources include (case insensitive):

 pod (po), service (svc), replicationcontroller (rc), deployment (deploy), replicaset (rs)

Examples:
  # Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000.
  kubectl expose rc nginx --port=80 --target-port=8000

  # Create a service for a replication controller identified by type and name specified in "nginx-controller.yaml",
which serves on port 80 and connects to the containers on port 8000.
  kubectl expose -f nginx-controller.yaml --port=80 --target-port=8000

  # Create a service for a pod valid-pod, which serves on port 444 with the name "frontend"
  kubectl expose pod valid-pod --port=444 --name=frontend

  # Create a second service based on the above service, exposing the container port 8443 as port 443 with the name
"nginx-https"
  kubectl expose service nginx --port=443 --target-port=8443 --name=nginx-https

  # Create a service for a replicated streaming application on port 4100 balancing UDP traffic and named 'video-stream'.
  kubectl expose rc streamer --port=4100 --protocol=UDP --name=video-stream

  # Create a service for a replicated nginx using replica set, which serves on port 80 and connects to the containers on
port 8000.
  kubectl expose rs nginx --port=80 --target-port=8000

  # Create a service for an nginx deployment, which serves on port 80 and connects to the containers on port 8000.
  kubectl expose deployment nginx --port=80 --target-port=8000

```

## 安装 dig 

```bash
yum install bind-utils	
```

```bash
# 先查询dns 地址，可以先运行一个容器，查看容器里边的 /etc/resolv.conf
dig -t A service-name.default.svc.cluster.local @dns地址
```

在容器里通过service域名访问

```bash
wget -O - -q http://nginx-service
```

## 加速docker配置

```bash
[root@master tomcat]# cat /etc/docker/daemon.json
{
  "registry-mirrors": [
    "https://dockerhub.azk8s.cn",
    "https://hub-mirror.c.163.com"
  ]
}
```

## kubectl scale

```bash
# Scale a replicaset named 'foo' to 3.
  kubectl scale --replicas=3 rs/foo

  # Scale a resource identified by type and name specified in "foo.yaml" to 3.
  kubectl scale --replicas=3 -f foo.yaml

  # If the deployment named mysql's current size is 2, scale mysql to 3.
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql

  # Scale multiple replication controllers.
  kubectl scale --replicas=5 rc/foo rc/bar rc/baz

  # Scale statefulset named 'web' to 3.
  kubectl scale --replicas=3 statefulset/web

```

试验

```bash
kubectl scale --replicas=2 deployment/nginx
```

## kubectl set

> 修改一些重要的参数，比如pod的image

```bash
kubectl set image deployment nginx nginx=nginx:latest
```

配置名为 `nginx` 的 deployment 把它的 `nginx` 容器的镜像修改为 `nginx:latest`

## kubectl rollout 

> 回滚

* 查看回滚状态

  ```bash
  kubectl rollout status deployment nginx
  ```

  ```bash
  [root@master ~]# kubectl set image deployment nginx nginx=nginx:alpine
  deployment.apps/nginx image updated
  [root@master ~]# kubectl rollout status deployment nginx
  Waiting for deployment "nginx" rollout to finish: 1 out of 3 new replicas have been updated...
  Waiting for deployment "nginx" rollout to finish: 1 out of 3 new replicas have been updated...
  Waiting for deployment "nginx" rollout to finish: 1 out of 3 new replicas have been updated...
  Waiting for deployment "nginx" rollout to finish: 2 out of 3 new replicas have been updated...
  Waiting for deployment "nginx" rollout to finish: 2 out of 3 new replicas have been updated...
  Waiting for deployment "nginx" rollout to finish: 2 out of 3 new replicas have been updated...
  Waiting for deployment "nginx" rollout to finish: 1 old replicas are pending termination...
  Waiting for deployment "nginx" rollout to finish: 1 old replicas are pending termination...
  deployment "nginx" successfully rolled out
  ```

* 执行回滚操作

  ```bash
  kubectl rollout undo deployment nginx
  ```

  ```bash
  [root@master ~]# kubectl rollout undo deployment nginx
  deployment.apps/nginx rolled back
  ```

# 资源对象

## workload

pod, ReplicaSet, Deployment, ReplicaController, StatefulSet, Job, DamonJob, Cronjob

## 服务发现和均衡

ingress, service

## 配置和存储

Volum , CSI

* ConfigMap, Secret
* Downward API

## 集群资源

Namespace, Node, ClusterRole, ClusterRoleBinding

## 元数据资源

HPA， PodTemplate, LimitRange

# 配置清单

apiVersion 格式是 group名/版本 如果group省略，group为 core

spec 指定这个资源应该有哪些属性，这些属性由相应的控制器实现

status 显示资源的当前状态，spec指定的用户期望的状态，k8s内部维护

apiServer仅接受JSON格式的资源定义，可以使用yaml格式提供资源清单，apiserver可以自动转换

`kubectl api-versions` 命令可以显示api的版本

alpha是内测版本，beta是公测版

metadata 元数据，管理这些资源的数据

​	labels, annotations, uid, name, namespace, ownerReferences, selfLink

apiserver中api的路径

​	`/api/GROUP/VERSION/namespaces/NAMESPACE/TYPE/NAME`

​	比如 `/api/v1/namespaces/default/pods/nginx-5c9b49c46-tsq52`

可以使用kubectl explain path命令查看

```bash
[root@master ~]# kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
```

## 定义一个Pod清单

> inline的都需要 {} 书写

pod-demo.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo
  namespace: default
  labels:
    run: demo
spec:
  containers:
  - name: myapp
    image: busybox:latest
    command:
    - "/bin/sh"
    - "-c"
    - "while true; do cat /etc/hostname > /usr/share/nginx/html/hostname.html; sleep 10s ;done"
  - name: nginx
    image: nginx:1.17.9
    ports: 
    - name: nginx-port
      containerPort: 80
```

然后直接运行 `kubectl create -f pod-demo.yml`

```bash
[root@master ~]# kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
client                  1/1     Running   0          117m
nginx-5c9b49c46-tsq52   1/1     Running   0          77m
pod-demo                2/2     Running   0          6m21s
```

`READY` 列，`2/2` 表示正常启动2个，总共有2个容器

## pod.spec.containers属性

* name

* image

* imagePullPolicy: Always(总是去仓库下载）, Never（如果没有就不用，有就用）, IfNotPresent（本地有就直接用，没有就去仓库下载）。如果image的tag是latest的，总是去仓库下载。

  Always能保证安全，因为本地的镜像可能被恶意修改过，然后通过 tag的方式，就会运行一些恶意的程序。

* ports: <[]object> 只是提供额外的信息，并不限制容器真实暴露的端口。

  * containerPort 容器内部的端口
  * hostIp 将此端口跟哪些IP地址绑定
  * hostPort
  * name
  * protocol

* command 默认要运行的程序，不会运行在shell中的。如果没有指定，则使用docker镜像的 entrypoint

* args 为command或者docker镜像的entrypoint传参。类似于docker的 `cmd` 指令。 `${variable}`变量替换

## 显示标签和标签过滤

```bash
kubectl get pods --show-labels # 查看所有标签
kubectl get pods -L app # 显示app标签
kubectl get pods -l app # 使用app标签过滤
kubectl get pods -l app,release # 使用多个标签过滤
kubectl get pods -l release=canary # 使用某个标签值
kubectl get pods -l release!=canary # 也可以不等于
kubectl get pods -l release in (canary,beta) # 也可以不等于
```

* 除此之外，还有对集合的 `in` , `notin` 

## 给资源打标签

```bash
kubectl label pods pod-demo app=test
```

如果这个标签已经有了，需要使用 `--overwrite` 选项

## 标签选择器

> 选择各种资源的语法，用于Service等

* matchLabels 直接给键值进行等值判断 `in`  `notin` `!=` `=` `exists` `notexists`

* matchExpressions 根据特定的表达式

  ```bash
  {key: "key", operator: "操作符", values: []}
  ```

  * 操作符有 `In`  `NotIn`  `Exists` `NotExists`

## pod.spec.nodeSelector

> 节点选择器  NodeSelector is a selector which must be true for the pod to fit on a node.

* diskType

## pod.spec.nodeName

> 将pod运行在指定名的节点上

## pod.metadata.annotations

不用于资源选择，仅为对象提供元数据

* 查看资源注解

# pod的生命周期

## 状态

https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/

### Pod phase 运行阶段

- 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。
- 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。
- 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。
- 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。
- 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。

### Pod status 状态

PodScheduled、Ready、Initialized 和 Unschedulable。

### 探针

- `livenessProbe`：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 [重启策略](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) 的影响。如果容器不提供存活探针，则默认状态为 `Success`。
- `readinessProbe`：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 `Failure`。如果容器不提供就绪探针，则默认状态为 `Success`

## 生命周期

1. 初始化

   1.1. Pending Running Failed Succeeded, Unkown ...

2. 主容器初始化

   2.1. postStart

   2.2. livenessProbe readnessProbe

   2.3. preStop

## 重启策略

restartPolicy

* Always 默认策略  5s, 10s, 30s ...
* OnFailure 
* Never

终止pod, 平滑缓冲30s , 30s还没结束就强制终止

## hostNetwork

使用宿主机的网络地址空间

## hostIPC hostPID

共享足迹的IPC，PID空间

## Pod探针

* ExecAction 命令返回的状态码
* TCPSocketAction
* HttpGet

定义在pod.spec.containers下。探针参数

1. initialDelaySeconds
2. periodSeconds
3. failureThreshold
4. successThreshold

### livenessProbe

### redinessProbe

### 试验

probe-demo.yml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: probe-demo
  namespace: default
  labels:
    app: demo
spec:
  containers:
  - name: probe-demo-container
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    command: ["/bin/sh","-c", "touch /tmp/health; sleep 30; rm -rf /tmp/health; sleep 36000"]
    livenessProbe:
      exec:
        command: ["test", "-e", "/tmp/health"]
      initialDelaySeconds: 1
      periodSeconds: 3
  restartPolicy: Never
```

对于httpGet的方式

```yaml
livenessProbe:
  httpGet:
    port: http_port #也可以是数字
    host: localhost
    path: /index.html
    header: xxx
```

为什么要做livenessProbe和readnessProbe？

> service在关联pod的时候，如果没有这两个探针，就会立即关联，导致服务不可用。

## postStart

容器创建后会立即执行这个钩子，如果这个方法失败了，容器创建失败，然后根据 `restartPolicy` 进行重启。

支持 `exec`  `httpGet`  `tcpSocket`

### 试验

pod-ps-demo.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: post-start-demo
  namespace: default
  labels:
    app: demo
spec:
  containers:
  - name: ps-container
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
    lifecycle: 
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo $HOSTNAME > /index.html"]
    command: ["/bin/httpd"]
    args: ["-f", "-h", "/"]
  restartPolicy: Never
```

## preStop

# Pod控制器

> 管理pod，使其状态维持到我们所期望的

## ReplicaSet

> 新一代的ReplicationController

用户期望的副本，标签选择器，pod模板

k8s但是建议我们不要直接使用ReplicaSet，建议使用Deployment。目前已经 `deprecated`

rs-demo.yaml

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-test
  namespace: default
  labels:
    rs: test
spec:
  replicas: 2
  selector: 
    matchLabels:
      app: demo
  template:
    metadata:
      name: pod-demo
      labels:
        app: demo
    spec:
      containers:
      - name: pod-container
        image: busybox:latest
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 80
        lifecycle:
          postStart:
            exec: 
              command: ["/bin/sh","-c", "echo $HOSTNAME > /index.html"]
        command: ["/bin/httpd", "-f", "-h", "/"]
```

可动态扩容

## Deployment

支持滚动更新，回滚操作，它控制的是ReplicaSet。用于无状态应用。将RS的 replicas逐渐修改，达到回滚和更新的目的。控制更新节奏和更新逻辑。更新回滚期间，最多多少个存活等。可以模拟出来，金丝雀，蓝绿，灰度部署。

* appVersion: apps/v1

* spec.strategy 更新策略
  * rollingUpdate 滚动更新
    * maxSurge 最多超出目标数几个，两种取值方式，`百分比` 和 `直接值` .比如原来5个，这个值设置的是20%，那可超出1个
    * maxUnavailable 最多有几个可用
  * type 发布类型
    * Recreate 重建更新 删一个建立一个
    * RollingUpdate 默认 滚动更新
* spec.revisionHistoryLimit 回滚的时候保留多少个历史版本
* spec.paused 暂停

deploy-test.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-test
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
      - name: myapp
        image: busybox:latest
        imagePullPolicy: IfNotPresent
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh", "-c", "echo $HOSTNAME > /index.html"]
        command: ["/bin/httpd", "-f", "-h", "/"]
```

* 可使用 `kubectl rollout history deployment deploy-test` 查看更新历史
* `kubectl rollout pause deployment deploy-test` 暂停更新。`kubectl rollout resume` 回复更新
* `kubectl rollout undo deployment deploy-test --to-revision=3` 回退到指定版本

## DaemonSet

> 确保每个节点都运行一个，始终运行在后台。比如收集日志。简称 `ds`

* updateStrategy
  * rollingUpdate 只能定义 `maxUnavailable` 只支持先删再更新
  * type
    * RollingUpdate
    * OnDelete

## Job

> 用户指定的数量，启动Pod资源，运行完成就推出。正常任务完成的Job不会被重建。用来执行一次性作业。

### CronJob

可周期性运行

## StatefulSet

> 部署有状态应用 

每一个都有状态，存储的。将应用部署定义成脚本，来执行

## service

可以使用控制器来管理pod资源，但是它不止使用一种控制器，这取决于它的选择器

## 更新的方式

* `kubectl patch` json格式

  ```bash
  kubectl patch deployment deploy-test -p '{"spec":{"replicas": 3}}'
  ```

* `kubectl edit`

* `kubectl set`

* `kubectl apply`

# Service

依赖一个重要组件，DNS（coreDNS)

三类网络空间

* `node network` 真实物理地址
* `pod network` pod的各容器之间
* `cluster network` pod之间

kube-proxy监控着etcd配置变动 `watch机制` 

## 调度方式

* userspace 代理调度 很多用户态到内核态的切换

  1.1之前

* iptables 

  ![](笔记.assets\image-20200312144355120.png)

* ipvs 在内核态直接调度，要额外添加一些属性，否则会自动降级到iptables. 在`/etc/sysconfig/kubelete` 里 `KUBELETE_EXTRA_ARGS="--fail-swap-on=false" KUBE_PROXY_MODE=ipvs` 还要装ipvs模块。具体再搜索。

  ![image-20200312144425323](笔记.assets\image-20200312144425323.png)

  kube-proxy watch到etcd的变化后，转换成ipvs规则，ipvs在内核态直接完成调度。

## 重要属性

### spec.type

* ExternalName 让集群访问外部的服务，像使用集群内部的服务一样。这个Name必须要被DNS解析。配合 `spec.externalName` 一起使用。这个名称应该是一个 `FQDN` 名称。
* ClusterIP 默认
* NodePort 可以在spec.ports里，指定nodePort端口。所有集群的节点都需要保证此钝口不被占用。映射过程 `client->NodeIP:NodePort->ClusterIP:servicePort->PodIP:containerPort`
* LoadBalancer 在NodePort上的增强功能

### spec.selector

如果 `spec.type=ExternalName` 这个字段被忽略， 如果不指定或者是空的话，此service被认为由外部进程管理Endpoints。

它就直接是pod的labels

## spec.sessionAffinity

来自于同一个客户端请求到同一个Pod。它有两个配置值 `ClientIP` `None`

## service dns记录

格式为 SVC_NAME.NS_NAME.DOMAIN.LTD

比如 ： `redis.default.svc.cluster.local`

## headless service

没有ClusterIP，直达管理的Pod. 将 `clusterIP`设置为`None` 。`serviceName->PodIP`

比如我们定义一个

```yaml
kind: Service
apiVersion: v1
metadata:
  name: headless-serve
spec:
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
  selector:
    app: demo
  clusterIP: None
  type: ClusterIP
  sessionAffinity: None
```

然后进入DNS查看 ，我们的corDNS地址是 `10.32.0.3`

```bash
dig -t A headless-serve.default.svc.cluster.local @10.32.0.3
```

我们发现下面三条DNS记录

```bash
headless-serve.default.svc.cluster.local. 30 IN A 10.32.0.6
headless-serve.default.svc.cluster.local. 30 IN A 10.32.0.9
headless-serve.default.svc.cluster.local. 30 IN A 10.32.0.10
```

这些地址分别对应

```bash
[root@master ~]# kubectl get pods -l app=demo -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP           NODE     
deploy-test-65b69645fc-lr79t   1/1     Running   0          3h34m   10.32.0.6    slave01 
deploy-test-65b69645fc-rjm4h   1/1     Running   0          3h23m   10.32.0.10   slave01 
deploy-test-65b69645fc-t2n4d   1/1     Running   0          3h34m   10.32.0.9    slave01 
```

可以看到是直连的

## 练习 

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: default
spec:
  selector: 
    app: redis
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
```

# IngressController

拥有7层代理能力的一组Pod资源，它不属于ControllerManager

* HAProxy
* Nginx
* Envoy
* Traefik 

如果直接使用nginx进行代理会有哪些问题？

1. 代理的Pod是有生命周期的，是会随时被替换，扩容的。
2. 必须要借助Service来实现，service仅仅用来分组和管理Pod

## Ingress 资源

不同于IngressController, Ingress通过Service来得到，Pod的改变，会被Ingress感知，然后直接注入到nginx配置中。

### 将nginx作为ingress

https://github.com/kubernetes/ingress-nginx

https://kubernetes.github.io/ingress-nginx/deploy/

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
```

就可以发现

```bash
ingress-nginx          nginx-ingress-controller-7f74f657bd-pbtnw    0/1     ContainerCreating   0          26s
```

已经被创建了

## 属性

* spec.rules 配置路由规则
  * host 全路径名，不能是IP地址
  * http
    * paths 
      * path
      * backend
        * serviceName 指定service必须是 headless
        * servicePort

## 制作证书

```bash
openssl genrsa -out my.key 2048
openssl req -new -x509 -key my.key -out my.crt -subject /C=CN/ST=SiChuan/L=Chendu/O=RD/CN=master.k8s
```

生成secret

```bash
kubectl create secret tls master-k8s-secret --cert=my.crt --key=my.key
```



## 练习1- 部署ingress服务

1. ingress.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-service
  namespace: default
spec:
  selector:
    app: demo
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: ClusterIP
  clusterIP: None
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata: 
  name: ingress-test
  namespace: default
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: master.k8s
    http:
      paths: 
      - path: /
        backend: 
          serviceName: ingress-service
          servicePort: 80
```

2. 添加接入外部流量

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml
```

它的内容如下

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
      nodePort: 30080
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
      nodePort: 30443
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
```

## 练习2：部署tomcat服务，并使用https

1. 创建证书

   ```bash
   openssl genrsa -out tomcat.key 2048
   openssl req -new -x509 -key tomcat.key -out tomcat.crt -subj /C=CN/ST=SiChuan/L=Chendu/O=RD/CN=tomcat.master.k8s
   ```

2. 创建secret

   ```bash
   kubectl create secret tls tomcat-tls-secret --cert=tomcat.crt --key=tomcat.key
   ```

3. 创建Deployment

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: tomcat-test
     namespace: default
   spec:
     selector:
       matchLabels:
         app: tomcat
     replicas: 3
     template:
       metadata:
         labels:
           app: tomcat
       spec:
         containers:
         - name: tomcat
           ports:
           - name: http
             port: 8080
             containerPort: 8080
           - name: https
             port: 443
             containerPort: 443
           imagePullPolicy: IfNotPresent
           image: tomcat:8-jdk13-openjdk-oracle  
   ```

4. 创建Service

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: tomcat-service
     namespace: default
   spec:
     selector:
       app: tomcat
     type: ClusterIP
     clusterIP: None
     ports:
     - name: http
       targetPort: 8080
       port: 8080
     - name: https
       targetPort: 443
       port: 443
   ```

5. 创建Ingress

   ```yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata: 
     name: ingress-tomcat
     namespace: default
     annotations:
       kubernetes.io/ingress.class: nginx
   spec:
     tls:
     - hosts:
       - tomcat.master.k8s
       secretName: tomcat-tls-secret
     rules:
     - host: tomcat.master.k8s
       http:
         paths: 
         - path: /
           backend: 
             serviceName: tomcat-service
             servicePort: 8080
   ```

# 存储卷

k8s来说，存储卷属于Pod，不属于单个容器

## emptyDir

随着pod删除，它也就会删除，可以当缓存使用

`kubectl explain pod.spec.volumes.emptyDir`

* medium 指定类型，空字符串意味着使用node的存储媒介。`Memory` 表示使用内存。
* sizeLimit 

## pod.spec.containers.volumeMounts



## hostPath

> 如果pod被调度到同一个目录里，这个数据是不会消失的。

节点目录

主流的分布式存储 `ceph` `cephfs` `hdfs` `glusterfs`

云存储 `EC2` `Azure Disk` `阿里云` minc 

type类型有：

* DirectoryOrCreate 不存在这个目录就创建
* Directory 必须存在的一个目录
* FileOrCreate
* File
* Socket 
* CharDevice
* BlockDevice

## persistentVolumeClaim

> 找一个跟node相关联的名称空间下声明的合适的pv

## gitRepo

本质上是emptyDir, 只是在启动的时候将git仓库克隆到本地目录来，克隆之后，git仓库的变化是不会同步到本地目录的。

## 练习

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-vc-test
  labels:
    app: vc-test
spec:
  containers:
  - name: httpd
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
    volumMounts:
    - name: html 
      mountPath: /data/web/html
    command: ["/bin/httpd", "-f", "-h /data/web/html"]
  - name: html
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    volumMounts:
    - name: html
      mountPath: /data
    command: ["/bin/sh", "-c", "mkdir -p /data; while true; do echo `date` > /data/index.html; sleep 1s; done"]
  volums:
  - name: html
    emptyDir: {}
```

## PVC

pv定义存储资源，pvc定义存储需求。pvc是有namespace的，而pv是cluster级别的

* accessModes 要跟pv匹配
* resources 
  * requests 需求多少
    * storage 

## PV

> 不是所有的存储卷都支持

* accessModes
  * ReadWriteOnce
  * ReadOnlyMany
  * ReadWriteMany
* capacity 定义容量
  
* storage
  
* persistentVolumeReclaimPolicy

  > 回收策略 Retain 保留，Recycle 回收，Delete 删除 . https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming

### 练习

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-test
spec:
  hostPath:
    path: /data/pv-test
    type: DirectoryOrCreate
  capacity:
    storage: 10Gi
  accessModes: ["ReadWriteMany", "ReadOnlyMany"]
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  namespace: default
  name: pvc-test-01
spec:
  accessModes: ["ReadWriteMany"]
  resources:
    requests:
      storage: 2Gi
```

## secret

特殊的configMap, 用另外一种编码方式存放

三种类型

1. tls 保存TLSsecret
2. generic 本地文件，目录或者字面值创建
3. docker-registry 创建使用docker-regsitry的密钥

ttl方式

```bash
kubectl create secret <type> <name> [--from-file=[key=]=source] [--from-literal=key=value]
```

## configMap

明文数据，配置信息管理，注入到pod容器里。它属于名称空间资源。简写 `cm`

```bash
kubectl create configmap <name> --from-literal=key1=xxx --from-literal=key2=xxx
```

`--from-file`如果不指定 `key` 就使用文件名作为键名，内容键值。

### 练习

一个nginx配置文件 test.conf

```ini
server {
  listen 80;
  server_name localhost;
  root /data/web/html;
}
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-conf
  namespace: default
data: 
  test.conf:
    server {
      listen 80;
      server_name localhost;
      root /data/web/html;
    }
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-test
  namespace: default
spec:
  containers:
  - name: nginx
    image: nginx:1.17.9
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
    volumeMounts:
    - mountPath: /etc/nginx/conf.d/
      name: nginx-conf
      readOnly: true
  volumes:
  - name: nginx-conf
    configMap:
      name: nginx-conf
```

## 配置容器化应用的方式

1. 自定义命令参数
2. 环境变量 用entrypoint脚本使用set命令取环境变量设置我们的配置信息等，cloudnative应用可以直接通过环境变量加载配置。只有启动才有效。
3. 把配置文件直接做到镜像里边去
4. 存储卷，将配置文件挂载到指定的文件目录中

## Pod获取环境变量的方式

1. env
   * value `$(VAR_NAME) `
   * valueFrom
     * configMapKeyRef `configMap`
     * fieldRef 引用pod的metadata字段
     * resourceFieldRef
     * secretKeyRef `Secret`
2. envFrom

# StatefulSet

有状态副本集

1. 稳定的网络标识符（IP地址不变）
2. 稳定持久的存储
3. 有序，平滑的部署和扩展
4. 有序，平滑的删除或终止

解析DNS格式 `pod_name.service_name.ns_name.svc.cluster.local`

每一个`pvc`的名称包含了`pod`的名称

`pod`的名称是固定的

`statefulset`也支持动态扩容 `kubectl scale sts name --replicas=3`

## 属性

1. updateStrategy 更新策略
   * rollingUpdate
     * partition 分区更新，partition:5 表示，编号大于等于5的更新。这样支持 `金丝雀` 发布

## 练习

1. 先准备pv

   ```yaml
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: pv-test-0
   spec:
     accessModes: ["ReadWriteMany", "ReadOnlyMany"]
     capacity:
       storage: 1Gi
     hostPath:
       path: /data/pv-0
       type: DirectoryOrCreate
   ---
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: pv-test-1
   spec:
     accessModes: ["ReadWriteMany", "ReadOnlyMany"]
     capacity:
       storage: 1Gi
     hostPath:
       path: /data/pv-1
       type: DirectoryOrCreate
   ---
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: pv-test-2
   spec:
     accessModes: ["ReadWriteMany", "ReadOnlyMany"]
     capacity:
       storage: 1Gi
     hostPath:
       path: /data/pv-2
       type: DirectoryOrCreate
   ```

2. 定义StatefulSet

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: sts-svc
   spec:
     type: ClusterIP
     clusterIP: None
     selector:
       app: sts-pod
     ports:
     - name: http
       port: 80
       targetPort: 80
   ---
   apiVersion: apps/v1
   kind: StatefulSet
   metadata:
     name: sts-test
   spec:
     serviceName: sts-svc
     selector: 
       matchLabels:
         app: sts-pod
     replicas: 3
     volumeClaimTemplates:
       - metadata:
          name: sts-pod-http
         spec:
           accessModes: ["ReadWriteMany"]
           resources:
             requests:
               storage: 500Mi
       - metadata:
           name: sts-pod-www
         spec:
           accessModes: ["ReadWriteMany"]
           resources:
             requests:
               storage: 500Mi
     template:
       metadata:
         name: sts-pod
         labels:
           app: sts-pod
       spec:
         containers:
         - name: nginx
           image: nginx:1.17.9
           imagePullPolicy: IfNotPresent
           ports:
           - name: http
             containerPort: 80
           volumeMounts:
           - name: sts-pod-http
             mountPath: /etc/nginx/conf.d/
           - name: sts-pod-www
             mountPath: /etc/nginx/html/
   ```

# 认证授权&访问控制

认证检查（authenticate）-授权检查（authorization）-准入控制（admission）

1. 认证令牌（token）
2. ssl 认证，服务器要认证客户端的身份
3. RBAC 授权检查

user, group, extra, api

api_path的格式 `/apis/api_group/api_version/namespaces/namespace_name/kind/kind_name`

http_verb: `get, post, put, delete, patch`

api_request_verb: `get, list, create, update, patch, watch, proxy, redirect, delete, deletecollection`

## 哪些组件需要访问API-Server

dashboard, 集群之外的api.

api有供集群内部使用的地址，和集群外部地址。

api-server的用户有两类，外部流量， pod

pod通过ServiceAccount同api-server进行认证交互。默认的default-token(名称空间里的)，只能让pod获取自身的相关信息。

## serviceaccount

通过`kubectl create serviceaccoun`t命令创建，系统自动生成 key, token信息

可以通过 `kubectl create xxx -o yaml --dry-run > xx.yaml` 来生成模板

也可以通过 `kubectl get pod pod-name -o yaml --export > xx.yaml`来导出配置

pod获取私有镜像的`imagePullSecret` 可以同 `serviceAccountName` 字段来替代，在sa里定义 imagepullsecret可以防止认证密钥定义在pod中。

```bash
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: null
  name: xiefq
```

## kubeconfig

kubectl config view

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.211.150:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
```

* contexts用于控制多个集群，这个是集群列表
* current-context 当前用哪个

切换用户

```bash
kubectl config use-context username@clustername
```

## 练习：创建登录用户

1. 创建登录认证

![image-20200326152840514](笔记.assets\image-20200326152840514.png)

2. 创建集群

![image-20200326152949930](笔记.assets/image-20200326152949930.png)

# RBAC

## roles

* operations 哪些操作

* objects 哪些对象

  限制资源的方式，某一类（kind)， 某一类指定的名称id

## rolebinding

* useraccount or service account
* role & clusterRole 都只能限制在rolebinding所在的namespace里。

## clusterRole



## clusterRoleBinding

## 几个比较重要的clusterrole

1. cluster-admin
2. admin

用rolebinding去绑定admin，就可以操作本集群里的所有资源。不用自己定义了。

## 练习

1. 创建role

   ```bash
   kubectl create role my-role --verb=get,list,watch --resource=pods -o yaml --dry-run
   ```

   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: Role
   metadata:
     creationTimestamp: "2020-03-17T23:44:28Z"
     name: my-role
     namespace: default
     resourceVersion: "432458"
     selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/default/roles/my-role
     uid: 301ca24c-9aec-4ab0-b695-f034b1c4d614
   rules:
   - apiGroups:
     - ""
     resources:
     - pods
     verbs:
     - get
     - list
     - watch
   ```

   可以观察到，清单的写法

2. 创建rolebinding

   ```bash
   kubectl create rolebinding bingding-xiefq --role=my-role --serviceaccount=default:xiefq -o yaml --dry-run
   ```

   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: RoleBinding
   metadata:
     creationTimestamp: null
     name: bingding-xiefq
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: Role
     name: my-role
   subjects:
   - kind: ServiceAccount
     name: xiefq
     namespace: default
   ```

# Helm

在k8s系统上部署应用

